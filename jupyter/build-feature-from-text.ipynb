{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seasonal-jenny",
   "metadata": {},
   "source": [
    "https://app.pluralsight.com/guides/building-features-from-text-data/\n",
    "\n",
    "# Building feature from text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-garage",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Text data is different from structured tabular data and, therefore, building features on it requires a completely different approach. In this guide, you will learn how to extract features from raw text for predictive modeling. You will also learn how to perform text preprocessing steps, and create Tf-Idf and Bag-of-words (BOW) feature matrices. We will begin by exploring the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-credit",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "In this guide, we will be using tweet data about the company 'Apple'. The objective is to create features that can be used for building a sentiment predictor model.\n",
    "\n",
    "The dataset contains 1181 observations and 3 variables, as described below:\n",
    "\n",
    "- Tweet: Consists of the twitter comments by the users. The twitter data is publicly available.\n",
    "\n",
    "- Avg: Average sentiment of the tweets (-2 means extremely negative while +2 means extremely positive). This classification was done using the Amazon Mechanical Turk.\n",
    "\n",
    "- Sentiment: Consists of the sentiment labels - positive, negative, and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-contact",
   "metadata": {},
   "source": [
    "## Loading the Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "provincial-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import string\n",
    "import nltk\n",
    "import warnings \n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-insulation",
   "metadata": {},
   "source": [
    "## Loading the Data and Performing Basic Data Checks\n",
    "\n",
    "The first line of code below reads in the data as pandas dataframe, while the second line prints the shape - 1,181 observations of 3 variables.\n",
    "\n",
    "The third line prints the first five observations.\n",
    "\n",
    "**WARNING** this dataset was not available to the lesson :( so it is derived from a partial example.\n",
    "\n",
    "(the following similar dataset was discovered online, but will need processing to work for this notebook)\n",
    "\n",
    "https://data.world/crowdflower/apple-twitter-sentiment/workspace/file?filename=Apple-Twitter-Sentiment-DFE.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coral-theta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3016, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Avg</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iphone 5c is ugly as heck what the freak @appl</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freak YOU @APPLE</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>freak you @apple</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@APPLE YOU RUINED MY LIFE</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@apple I hate apple!!!!!</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Tweet  Avg Sentiment\n",
       "0  iphone 5c is ugly as heck what the freak @appl -2.0  Negative\n",
       "1                                freak YOU @APPLE -2.0  Negative\n",
       "2                                freak you @apple -2.0  Negative\n",
       "3                       @APPLE YOU RUINED MY LIFE -2.0  Negative\n",
       "4                        @apple I hate apple!!!!! -2.0  Negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = pd.read_csv('datatweets.csv')\n",
    "print(dat.shape)\n",
    "dat.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-diamond",
   "metadata": {},
   "source": [
    "We will start by performing basic analysis of the data. The line of code below prints the number of tweets, as per the 'Sentiment' label. The output shows that the highest number of tweets are for the negative sentiment, while the lowest are for the positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "included-miracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    1098\n",
       "Neutral     1549\n",
       "Positive     369\n",
       "Name: Tweet, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of dates / entries in each month\n",
    "dat.groupby('Sentiment')['Tweet'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-swedish",
   "metadata": {},
   "source": [
    "The sentiment score for the tweets is stored in the variable 'Avg', that ranges from -2 (extremely negative) to +2 (extremely positive). We will explore if there is a difference in the average sentiment scores across the 'sentiment' label. The line of code below performs this task and the output shows that the average negative score is -0.74, while the average positive score is 0.57."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "equal-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative   -2.0\n",
       "Neutral     0.0\n",
       "Positive    2.0\n",
       "Name: Avg, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.groupby('Sentiment')['Avg'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-walker",
   "metadata": {},
   "source": [
    "# Building Simple Features from Raw Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-natural",
   "metadata": {},
   "source": [
    "Many simple but important features can be extracted from the raw text data, as discussed below.\n",
    "\n",
    "## Character Length\n",
    "\n",
    "The hypothesis is that the length of the characters in a tweet varies across the sentiment it carries. The first line of code below creates a new variable 'character_cnt' that takes in the text from the 'Tweet' variable and calculates the count of the characters in the text. The second line performs the 'groupby' operation on the 'Sentiment' label and prints the average character length across the labels.\n",
    "\n",
    "The output shows that the neutral sentiments have a lower character count on an average, as compared to the positive and the negative tweets. This inference can be useful for separating the neutral tweets from the other types of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unexpected-blocking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    86.949909\n",
       "Neutral     81.709490\n",
       "Positive    84.891599\n",
       "Name: character_cnt, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['character_cnt'] = dat['Tweet'].str.len()\n",
    "dat.groupby('Sentiment')['character_cnt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-financing",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "Just like the character count in a tweet, the word count can also be a useful feature. The first line of code below creates a new variable 'word_counts' that takes in the text from the 'Tweet' variable and calculates the count of the words in the text. The second line performs the 'groupby' operation on the 'Sentiment' label and prints the average word length across the labels.\n",
    "\n",
    "The output shows that the negative sentiments have the highest average word count, suggesting that the disappointed customers tend to write longer tweets. This inference can be useful for separating the 'sentiment' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "atlantic-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    15.271403\n",
       "Neutral     13.078115\n",
       "Positive    13.945799\n",
       "Name: word_counts, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['word_counts'] = dat['Tweet'].str.split().str.len()\n",
    "dat.groupby('Sentiment')['word_counts'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-bidder",
   "metadata": {},
   "source": [
    "## Average Character Length per Word\n",
    "\n",
    "Since we have created the 'character_cnt' and the 'word_counts' features, it is easy to create the ratio of these two variables that will give the average length of the character per word in each tweet.\n",
    "\n",
    "The first line of code below creates a new variable 'characters_per_word' that is the ratio of the number of characters and the number of words in a tweet. The second line performs the 'groupby' operation on the 'Sentiment' label and prints the average character length per word across the labels.\n",
    "\n",
    "The output shows that neutral sentiments have the highest average character length per word. This inference can be useful for separating the 'Sentiment' labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "essential-absorption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    5.792099\n",
       "Neutral     6.390818\n",
       "Positive    6.227200\n",
       "Name: characters_per_word, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['characters_per_word'] = dat['character_cnt']/dat['word_counts']\n",
    "dat.groupby('Sentiment')['characters_per_word'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-programmer",
   "metadata": {},
   "source": [
    "## Special Character Count\n",
    "\n",
    "It is also possible to create a feature that contains the count of special characters like '@' or '#'. The first line of code below creates a new feature 'spl' that takes in the text from the 'Tweet' variable and calculates the count of the words starting with the special character '@'. We use the starts with function for performing this operation. The second line prints the first five observations containing the 'Tweet' and the 'spl' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "processed-discharge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>spl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iphone 5c is ugly as heck what the freak @appl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>freak YOU @APPLE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>freak you @apple</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@APPLE YOU RUINED MY LIFE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@apple I hate apple!!!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Tweet  spl\n",
       "0  iphone 5c is ugly as heck what the freak @appl    1\n",
       "1                                freak YOU @APPLE    1\n",
       "2                                freak you @apple    1\n",
       "3                       @APPLE YOU RUINED MY LIFE    1\n",
       "4                        @apple I hate apple!!!!!    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['spl'] = dat['Tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('@')]))\n",
    "dat[['Tweet','spl']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-compiler",
   "metadata": {},
   "source": [
    "## Number Count\n",
    "\n",
    "Just like we created a feature on the count of words in a tweet, we can also create a feature on the count of numbers in a tweet. The first line of code below creates a new variable 'num' that takes in the text from the 'Tweet' variable and calculates the count of the numbers in the text. The second line performs the 'groupby' operation on the 'Sentiment' label and prints the average count of numbers across the labels.\n",
    "\n",
    "The output shows that the neutral sentiment labels have the lowest average count of numbers in a tweet, whereas the negative tweets have the highest average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aware-contamination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    0.151184\n",
       "Neutral     0.213686\n",
       "Positive    0.192412\n",
       "Name: num, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of numerics\n",
    "dat['num'] = dat['Tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "dat.groupby('Sentiment')['num'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-increase",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-arbitration",
   "metadata": {},
   "source": [
    "# Pre-processing the Raw Text\n",
    "\n",
    "So far, we have created simple features from the raw text. We can also create more advanced features but, before that, we will have to clean the text. The common pre-processing steps are summarized below:\n",
    "\n",
    "1. Removing punctuation - the rule of thumb is to remove everything that is not in the form x,y,z. The first line of code below performs this task.\n",
    "\n",
    "1. Removing stopwords - these are unhelpful words like 'the', 'is', 'at'. These are not helpful because the frequency of such stopwords is high in the corpus, but they don't help in differentiating the target classes. The removal of Stopwords also reduces the data size. The second line of code below performs this task.\n",
    "\n",
    "1. Conversion to lowercase - words like 'Phone' and 'phone' need to be considered as one word. Hence, these are converted to lowercase. The third line of code below performs this task.\n",
    "\n",
    "1. Stemming - the goal of stemming is to reduce the number of inflectional forms of words appearing in the text. This causes words such as “argue”, \"argued\", \"arguing\", \"argues\" to be reduced to their common stem “argu”. There are many ways to perform Stemming, the popular one being the “Porter Stemmer” method by Martin Porter. The fourth to sixth lines of code below perform this task.\n",
    "\n",
    "The last line of code prints a summary of all the new features that we have built so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rotary-chrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-33935a427b35>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dat['processedtext'] = dat['Tweet'].str.replace('[^\\w\\s]','')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_cnt</th>\n",
       "      <th>word_counts</th>\n",
       "      <th>characters_per_word</th>\n",
       "      <th>spl</th>\n",
       "      <th>num</th>\n",
       "      <th>processedtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>iphon 5c ugli heck freak appl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>freak you appl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>freak appl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>appl you ruin my life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>appl i hate appl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   character_cnt  word_counts  characters_per_word  spl  num  \\\n",
       "0             46           10             4.600000    1    0   \n",
       "1             16            3             5.333333    1    0   \n",
       "2             16            3             5.333333    1    0   \n",
       "3             25            5             5.000000    1    0   \n",
       "4             24            4             6.000000    1    0   \n",
       "\n",
       "                   processedtext  \n",
       "0  iphon 5c ugli heck freak appl  \n",
       "1                 freak you appl  \n",
       "2                     freak appl  \n",
       "3          appl you ruin my life  \n",
       "4               appl i hate appl  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['processedtext'] = dat['Tweet'].str.replace('[^\\w\\s]','') \n",
    "dat['processedtext'] = dat['processedtext'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "dat['processedtext'] = dat['processedtext'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "\n",
    "#Lines 4 to 6\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "dat['processedtext'] = dat['processedtext'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "dat[['character_cnt','word_counts','characters_per_word', 'spl', 'num', 'processedtext']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-scanning",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-warner",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency (TF-IDF) Vector\n",
    "\n",
    "We have cleaned the text which is now stored in a new variable 'processedtext'. However, in order to use it for building machine learning models, we will have to convert it to word frequency vectors.\n",
    "\n",
    "One of the most popular methods to do this is through the TF-IDF representation, which is used as a weighting factor in text mining applications. In simple terms, TF-IDF attempts to highlight important words which appear frequently in a document but not across documents. The terms are briefly explained below:\n",
    "\n",
    "1. Term Frequency (TF): This summarizes the normalized Term Frequency within a document.\n",
    "\n",
    "1. Inverse Document Frequency (IDF): This reduces the weight of terms that appear a lot across documents.\n",
    "\n",
    "Now, we will work on creating the TF-IDF vectors for our tweets. The first line of code below imports the 'TfidfVectorizer' from sklearn.feature_extraction.text module. The second line initializes the TfidfVectorizer object, called 'tfidf', while the third line fits and transforms the variable 'processedtext' from the data.\n",
    "\n",
    "The important arguments we have used in initiating the TfidfVectorizer object are the 'max_features' and the 'ngram_range'. While the 'max_features' argument specifies the maximum number of features to be created, the argument 'ngram_range=(1,1)' specifies that unigrams will be considered for feature creation.\n",
    "\n",
    "The fourth line prints a summary of the object, which is a sparse matrix containing the number of observations (1181) and the number of features (500).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "expensive-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 38)\t0.33367517418709947\n",
      "  (0, 218)\t0.9426881128619417\n",
      "  (1, 38)\t1.0\n",
      "  (2, 38)\t1.0\n",
      "  (3, 248)\t0.9811380155172429\n",
      "  (3, 38)\t0.193308547423249\n",
      "  (4, 189)\t0.9282015014964591\n",
      "  (4, 38)\t0.3720779120288103\n",
      "  (5, 438)\t0.37834361403311906\n",
      "  (5, 484)\t0.4200934898559464\n",
      "  (5, 188)\t0.42352816849143876\n",
      "  (5, 432)\t0.45948873321342304\n",
      "  (5, 433)\t0.3061266986594431\n",
      "  (5, 466)\t0.3765150190752095\n",
      "  (5, 38)\t0.07782178465306747\n",
      "  (5, 218)\t0.21985954302076122\n",
      "  (6, 119)\t0.3528775517743811\n",
      "  (6, 278)\t0.42530636574548564\n",
      "  (6, 39)\t0.48869488180537557\n",
      "  (6, 273)\t0.47784768113907605\n",
      "  (6, 466)\t0.40541886716797154\n",
      "  (6, 38)\t0.08379591298251557\n",
      "  (6, 218)\t0.23673745362529616\n",
      "  (7, 381)\t0.5838288184018563\n",
      "  (7, 363)\t0.37800913047565576\n",
      "  :\t:\n",
      "  (3011, 82)\t0.5873363377028471\n",
      "  (3011, 262)\t0.2470472603379669\n",
      "  (3011, 38)\t0.06439096954426193\n",
      "  (3011, 218)\t0.18191524650555752\n",
      "  (3012, 246)\t0.45192301735774204\n",
      "  (3012, 383)\t0.38897452405245014\n",
      "  (3012, 129)\t0.45953297622307376\n",
      "  (3012, 163)\t0.3824672494398225\n",
      "  (3012, 311)\t0.41535077329375575\n",
      "  (3012, 314)\t0.32588701433722206\n",
      "  (3012, 38)\t0.09107163060248101\n",
      "  (3013, 496)\t0.545537312531455\n",
      "  (3013, 492)\t0.5044700696168605\n",
      "  (3013, 262)\t0.38133918694157626\n",
      "  (3013, 188)\t0.5409255217707517\n",
      "  (3013, 38)\t0.09939312801444168\n",
      "  (3014, 448)\t0.7528289838092909\n",
      "  (3014, 383)\t0.6408844873619317\n",
      "  (3014, 38)\t0.15005197431391679\n",
      "  (3015, 385)\t0.5148168437087275\n",
      "  (3015, 260)\t0.4300860755953713\n",
      "  (3015, 237)\t0.4283319095622074\n",
      "  (3015, 400)\t0.39931106232233043\n",
      "  (3015, 466)\t0.44563238177642867\n",
      "  (3015, 38)\t0.09210763314095421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=500, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\n",
    "\n",
    "dat_tfIdf = tfidf.fit_transform(dat['processedtext'])\n",
    "print(dat_tfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-amber",
   "metadata": {},
   "source": [
    "# Bag-of-words Vector\n",
    "\n",
    "Another popular technique for creating word vectors is the Bag-of-words approach. It is a simplistic method for identifying topics in a document. It works on the assumption that the higher the frequency of the term, the higher its importance.\n",
    "\n",
    "The first line of code below imports the 'CountVectorizer' utility from the 'sklearn.feature_extraction.text' module. The second line initializes the CountVectorizer object, called 'bag_words', while the third line fits and transforms the variable 'processedtext' from the data. The fourth line prints a summary of the object, which is, again, a sparse matrix containing the number of observations (1181) and the number of features (500)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "outer-commissioner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3016x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 18217 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bag_words = CountVectorizer(max_features=500, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\n",
    "dat_BOW = bag_words.fit_transform(dat['processedtext'])\n",
    "dat_BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-traffic",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this guide, you have learned the fundamentals of building features from the raw and the processed text data. You can now use the basic as well as advanced features for building a machine learning algorithm that can predict the sentiment of a tweet.\n",
    "\n",
    "- [Natural Language Processing - Machine Learning with Text Data](https://app.pluralsight.com/guides/nlp-machine-learning-text-data/)\n",
    "- [Natural Language Processing – Text Parsing](https://app.pluralsight.com/guides/text-parsing/)\n",
    "- [Natural Language Processing - Topic Identification](https://app.pluralsight.com/guides/topic-identification-nlp/)\n",
    "- [Visualizing Text Data Using a Word Cloud](https://app.pluralsight.com/guides/natural-language-processing-visualizing-text-data-using-word-cloud/)\n",
    "- [Named Entity Recognition (NER)](https://app.pluralsight.com/guides/natural-language-processing-named-entity-recognition/)\n",
    "- [Natural Language Processing - Extracting Sentiment from the Text Data](https://app.pluralsight.com/guides/natural-language-processing-extracting-sentiment-from-text-data/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
